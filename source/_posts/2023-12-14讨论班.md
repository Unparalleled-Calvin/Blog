---
title: 2023-12-14讨论班
tags: [LLM]
categories: [组会记录]
date: 2023-12-14 18:50:26
---

#### 从基础架构角度看大模型推理优化

模型侧

- 量化——将以前高bit数存储的数据转为低bit数存储的数据，例如用16bit short代替32bit float
  - 能放下的参数更多
  - 数据传输速度更快
- 稀疏化
  - 在参数矩阵中一些位置挖空置0，从而以稀疏矩阵的形式传输，Nvidia支持满足一定约束的稀疏矩阵计算
- 蒸馏
  - 得到的小模型能够表现出与大模型类似的能力
- KV cache

系统侧

- 并行化【iterative infer带来的革新】
  - 大模型时代，经验上来看计算量越大，最匹配的语料+参数得到的Loss越好，但是内存/通信开销成为了瓶颈
  - 数据并行/张量并行/流水线并行 见{% post_link 2023-11-30讨论班 %}
  - 传统的CV模型可能只做第一个并行，后两者由于CV网络连接方式多元、模块之间计算量不平衡等问题难以做到，但是大模型由于都是Transformer，做起来就很容易

自回归推理引擎

推理的第一步是把不完整的句子经过prefill转为向量

在batch的过程中仍然有early stop和late arrive等问题，因为是一个词一次迭代，batch中的多个句子最终形态可能词数不一致

- iteration-level schedule
- prefill以及generate的过程中有一些是request无关的计算，把在这些步骤batch里的数据拼到一起【如果模型对向量中的数据不敏感的话】，在attention这种request有关的东西执行时再拆开



KV cache

- 传统的bert每次都需要计算之前的信息
- 存储之前所有词对应的value，从而在迭代推理过程中不用重复计算



Memory Management

GPU借鉴Linux的COW思想

